{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical"
      ],
      "metadata": {
        "id": "VQXf3FTGLV-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gKOnnHZWLYEx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf617784"
      },
      "source": [
        "**1. What is a Decision Tree, and how does it work?**\n",
        "A Decision Tree is a supervised machine learning algorithm that uses a tree-like structure to model decisions and their possible consequences. It works by recursively partitioning the data based on features to create branches, leading to a final decision (or prediction) at the leaf nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f8f70d9"
      },
      "source": [
        "**2. What are impurity measures in Decision Trees?**\n",
        "Impurity measures in Decision Trees are metrics used to determine the homogeneity of the data at a particular node. They quantify how mixed the classes are within a subset of data. Lower impurity indicates that the data at a node is more uniform in terms of the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147a830d"
      },
      "source": [
        "**3. What is the mathematical formula for Gini Impurity?**\n",
        "The mathematical formula for Gini Impurity for a node is:\n",
        "$Gini = 1 - \\sum_{i=1}^{C} (p_i)^2$\n",
        "where $C$ is the number of classes and $p_i$ is the proportion of instances belonging to class $i$ at that node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "554264d1"
      },
      "source": [
        "**4. What is the mathematical formula for Entropy?**\n",
        "The mathematical formula for Entropy for a node is:\n",
        "$Entropy = -\\sum_{i=1}^{C} p_i \\log_2(p_i)$\n",
        "where $C$ is the number of classes and $p_i$ is the proportion of instances belonging to class $i$ at that node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "704e7235"
      },
      "source": [
        "**5. What is Information Gain, and how is it used in Decision Trees?**\n",
        "Information Gain is a measure used in Decision Trees to determine the effectiveness of splitting a node based on a particular feature. It quantifies the reduction in entropy (or Gini impurity) achieved by the split. Decision Trees use Information Gain to select the best feature to split on at each step, aiming to maximize the information gain and create more homogeneous child nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ee15f9"
      },
      "source": [
        "**6. What is the difference between Gini Impurity and Entropy?**\n",
        "Both Gini Impurity and Entropy are measures of impurity used in Decision Trees. The main difference lies in their mathematical formulas and how they penalize impurity. Entropy tends to penalize impurity more heavily than Gini Impurity and is based on logarithmic calculations, while Gini Impurity is based on squared probabilities. In practice, they often yield similar results in terms of tree structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8f28f34"
      },
      "source": [
        "**7. What is the mathematical explanation behind Decision Trees?**\n",
        "The mathematical explanation behind Decision Trees revolves around finding the optimal sequence of splits to minimize impurity and maximize information gain. At each node, the algorithm considers splitting the data based on different features and thresholds (for continuous features). It calculates the impurity (Gini or Entropy) before and after the potential split and chooses the split that results in the highest information gain. This process is repeated recursively until a stopping criterion is met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f733e138"
      },
      "source": [
        "**8. What is Pre-Pruning in Decision Trees?**\n",
        "Pre-pruning is a technique used to prevent overfitting in Decision Trees by stopping the tree growth prematurely during the training phase. This is done by setting constraints or criteria that, when met, prevent further splitting of a node. Examples of pre-pruning criteria include maximum tree depth, minimum samples required to split a node, or minimum samples required at a leaf node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adabd4da"
      },
      "source": [
        "**9. What is Post-Pruning in Decision Trees?**\n",
        "Post-pruning is a technique used to prevent overfitting in Decision Trees by pruning (removing) branches from a fully grown tree. After the tree has been built to its maximum depth, branches are removed if they do not contribute significantly to improving the model's performance on a validation set. This is often done by evaluating the impact of removing a subtree on the model's accuracy or another performance metric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b190b3e"
      },
      "source": [
        "**10. What is the difference between Pre-Pruning and Post-Pruning?**\n",
        "The main difference between Pre-pruning and Post-pruning is when they are applied. Pre-pruning stops the tree growth during the training phase, while Post-pruning prunes branches from a fully grown tree after training. Pre-pruning is generally faster but might stop the tree too early, potentially missing important patterns. Post-pruning is computationally more expensive as it involves building a full tree first, but it often results in a more optimal tree structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7e09a19"
      },
      "source": [
        "**11. What is a Decision Tree Regressor?**\n",
        "A Decision Tree Regressor is a variation of the Decision Tree algorithm used for regression tasks (predicting continuous values) instead of classification tasks (predicting discrete classes). Instead of predicting a class at the leaf nodes, a Decision Tree Regressor predicts a numerical value, typically the average or median of the target variable for the training instances that fall into that leaf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e07c205"
      },
      "source": [
        "**12. What are the advantages and disadvantages of Decision Trees?**\n",
        "**Advantages:**\n",
        "*   Easy to understand and interpret.\n",
        "*   Can handle both numerical and categorical data.\n",
        "*   Require minimal data preprocessing (e.g., no need for feature scaling).\n",
        "*   Can capture non-linear relationships between features and the target variable.\n",
        "\n",
        "**Disadvantages:**\n",
        "*   Prone to overfitting, especially with complex trees.\n",
        "*   Can be unstable; small changes in the data can lead to significantly different tree structures.\n",
        "*   Can create biased trees if some classes dominate the dataset.\n",
        "*   Can struggle with datasets that have many features or complex relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bf45218"
      },
      "source": [
        "**13. How does a Decision Tree handle missing values?**\n",
        "There are several ways Decision Trees can handle missing values. Some implementations can handle them intrinsically by using surrogate splits (using alternative features to make decisions when the primary feature is missing). Other approaches involve imputing missing values before training the tree, using techniques like mean, median, or mode imputation, or more advanced methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ea4fb5"
      },
      "source": [
        "**14. How does a Decision Tree handle categorical features?**\n",
        "Decision Trees can handle categorical features by creating splits based on the categories. For a categorical feature, the tree can create a branch for each category or group categories together to create binary splits. The method used depends on the specific Decision Tree algorithm and implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f959c5ad"
      },
      "source": [
        "**15. What are some real-world applications of Decision Trees?**\n",
        "Decision Trees are used in various real-world applications, including:\n",
        "*   **Medical diagnosis:** To help diagnose diseases based on patient symptoms and medical history.\n",
        "*   **Financial analysis:** For credit risk assessment, stock market prediction, and fraud detection.\n",
        "*   **Customer relationship management (CRM):** For customer segmentation, churn prediction, and targeted marketing.\n",
        "*   **Manufacturing:** For quality control and identifying factors that affect product defects.\n",
        "*   **Bioinformatics:** For gene expression analysis and protein structure prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "practical"
      ],
      "metadata": {
        "id": "LvgTKbfLNc9F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GFshyU33NfIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bcb3a72"
      },
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d98c34f4",
        "outputId": "81c6f085-cd4a-4635-d1cc-2f80f16843f7"
      },
      "source": [
        "# Install necessary libraries\n",
        "%pip install scikit-learn pandas"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fadca6a5",
        "outputId": "5990f862-d490-4dd2-a742-3e8a061d0f6b"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y_iris = pd.Series(iris.target)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b982b912"
      },
      "source": [
        "17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f0f5bd4",
        "outputId": "2fdf19b6-fcbd-4c93-8e49-b3606f00af03"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Create an instance of DecisionTreeClassifier with Gini impurity\n",
        "dt_classifier_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier on the Iris dataset (using the full dataset for feature importance)\n",
        "dt_classifier_gini.fit(X_iris, y_iris)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_classifier_gini.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "feature_importance_series = pd.Series(feature_importances, index=X_iris.columns)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature importances using Gini Impurity:\")\n",
        "print(feature_importance_series.sort_values(ascending=False))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances using Gini Impurity:\n",
            "petal length (cm)    0.564056\n",
            "petal width (cm)     0.422611\n",
            "sepal length (cm)    0.013333\n",
            "sepal width (cm)     0.000000\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8484c618"
      },
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23de92bd",
        "outputId": "3d703705-88ea-4ae7-8195-3f8c1be75886"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets (if not already split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Create an instance of DecisionTreeClassifier with Entropy\n",
        "dt_classifier_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "dt_classifier_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred_entropy = dt_classifier_entropy.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "print(f\"Accuracy of Decision Tree Classifier using Entropy: {accuracy_entropy:.2f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier using Entropy: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0fe25fb"
      },
      "source": [
        "19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aef37463",
        "outputId": "91867fdf-f9bb-4add-a817-469bf64c65f8"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y_housing = pd.Series(housing.target)\n",
        "\n",
        "# Create an instance of DecisionTreeRegressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor on the housing dataset\n",
        "dt_regressor.fit(X_housing, y_housing)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_housing = dt_regressor.predict(X_housing)\n",
        "\n",
        "# Calculate and print Mean Squared Error\n",
        "mse_housing = mean_squared_error(y_housing, y_pred_housing)\n",
        "print(f\"Mean Squared Error of Decision Tree Regressor on Housing dataset: {mse_housing:.2f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Decision Tree Regressor on Housing dataset: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83f9d07b"
      },
      "source": [
        "20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6451f8de",
        "outputId": "ea40d4c7-c085-4716-bd82-71e67a4159be"
      },
      "source": [
        "# Install graphviz\n",
        "%pip install graphviz"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "1d2122a4",
        "outputId": "aa7f605f-32f8-4bdb-a71d-2c79525c134d"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Train a Decision Tree Classifier (using Gini for consistency with previous example)\n",
        "dt_classifier_viz = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier_viz.fit(X_iris, y_iris)\n",
        "\n",
        "# Export the tree to a DOT file\n",
        "dot_data = export_graphviz(dt_classifier_viz, out_file=None,\n",
        "                           feature_names=X_iris.columns,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a graph from the DOT data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Render the graph (this will display the tree)\n",
        "graph"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"863pt\" height=\"671pt\"\n viewBox=\"0.00 0.00 863.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 859,-667 859,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M517,-663C517,-663 382,-663 382,-663 376,-663 370,-657 370,-651 370,-651 370,-592 370,-592 370,-586 376,-580 382,-580 382,-580 517,-580 517,-580 523,-580 529,-586 529,-592 529,-592 529,-651 529,-651 529,-657 523,-663 517,-663\"/>\n<text text-anchor=\"start\" x=\"378\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"414\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"start\" x=\"404.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n<text text-anchor=\"start\" x=\"391.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n<text text-anchor=\"start\" x=\"406\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M419,-536.5C419,-536.5 326,-536.5 326,-536.5 320,-536.5 314,-530.5 314,-524.5 314,-524.5 314,-480.5 314,-480.5 314,-474.5 320,-468.5 326,-468.5 326,-468.5 419,-468.5 419,-468.5 425,-468.5 431,-474.5 431,-480.5 431,-480.5 431,-524.5 431,-524.5 431,-530.5 425,-536.5 419,-536.5\"/>\n<text text-anchor=\"start\" x=\"344.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"331.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n<text text-anchor=\"start\" x=\"322\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n<text text-anchor=\"start\" x=\"329\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M422.79,-579.91C415.38,-568.65 407.33,-556.42 399.88,-545.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"402.75,-543.1 394.33,-536.67 396.9,-546.94 402.75,-543.1\"/>\n<text text-anchor=\"middle\" x=\"389.28\" y=\"-557.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M591.5,-544C591.5,-544 461.5,-544 461.5,-544 455.5,-544 449.5,-538 449.5,-532 449.5,-532 449.5,-473 449.5,-473 449.5,-467 455.5,-461 461.5,-461 461.5,-461 591.5,-461 591.5,-461 597.5,-461 603.5,-467 603.5,-473 603.5,-473 603.5,-532 603.5,-532 603.5,-538 597.5,-544 591.5,-544\"/>\n<text text-anchor=\"start\" x=\"457.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"498.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n<text text-anchor=\"start\" x=\"472\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n<text text-anchor=\"start\" x=\"474\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M476.21,-579.91C482.01,-571.1 488.2,-561.7 494.18,-552.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"497.26,-554.3 499.83,-544.02 491.41,-550.45 497.26,-554.3\"/>\n<text text-anchor=\"middle\" x=\"504.88\" y=\"-564.81\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#4de88e\" stroke=\"black\" d=\"M482,-425C482,-425 347,-425 347,-425 341,-425 335,-419 335,-413 335,-413 335,-354 335,-354 335,-348 341,-342 347,-342 347,-342 482,-342 482,-342 488,-342 494,-348 494,-354 494,-354 494,-413 494,-413 494,-419 488,-425 482,-425\"/>\n<text text-anchor=\"start\" x=\"343\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"379\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n<text text-anchor=\"start\" x=\"373.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n<text text-anchor=\"start\" x=\"364\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n<text text-anchor=\"start\" x=\"362\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M487.64,-460.91C478.87,-451.74 469.47,-441.93 460.44,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"462.73,-429.82 453.29,-425.02 457.68,-434.66 462.73,-429.82\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#843de6\" stroke=\"black\" d=\"M706,-425C706,-425 571,-425 571,-425 565,-425 559,-419 559,-413 559,-413 559,-354 559,-354 559,-348 565,-342 571,-342 571,-342 706,-342 706,-342 712,-342 718,-348 718,-354 718,-354 718,-413 718,-413 718,-419 712,-425 706,-425\"/>\n<text text-anchor=\"start\" x=\"567\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"603\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n<text text-anchor=\"start\" x=\"597.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n<text text-anchor=\"start\" x=\"588\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n<text text-anchor=\"start\" x=\"590\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>2&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M565.36,-460.91C574.13,-451.74 583.53,-441.93 592.56,-432.49\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"595.32,-434.66 599.71,-425.02 590.27,-429.82 595.32,-434.66\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#3de684\" stroke=\"black\" d=\"M260.5,-306C260.5,-306 130.5,-306 130.5,-306 124.5,-306 118.5,-300 118.5,-294 118.5,-294 118.5,-235 118.5,-235 118.5,-229 124.5,-223 130.5,-223 130.5,-223 260.5,-223 260.5,-223 266.5,-223 272.5,-229 272.5,-235 272.5,-235 272.5,-294 272.5,-294 272.5,-300 266.5,-306 260.5,-306\"/>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.65</text>\n<text text-anchor=\"start\" x=\"160\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.041</text>\n<text text-anchor=\"start\" x=\"154.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 1]</text>\n<text text-anchor=\"start\" x=\"143\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.52,-341.91C319.75,-331.88 299.52,-321.07 280.36,-310.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"281.82,-307.65 271.35,-306.02 278.52,-313.82 281.82,-307.65\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M479.5,-306C479.5,-306 349.5,-306 349.5,-306 343.5,-306 337.5,-300 337.5,-294 337.5,-294 337.5,-235 337.5,-235 337.5,-229 343.5,-223 349.5,-223 349.5,-223 479.5,-223 479.5,-223 485.5,-223 491.5,-229 491.5,-235 491.5,-235 491.5,-294 491.5,-294 491.5,-300 485.5,-306 479.5,-306\"/>\n<text text-anchor=\"start\" x=\"345.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"379\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"377\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"367.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"366\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M414.5,-341.91C414.5,-333.65 414.5,-324.86 414.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"418,-316.02 414.5,-306.02 411,-316.02 418,-316.02\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 47</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.66,-222.91C135.04,-211.1 120.17,-198.22 106.6,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108.62,-183.57 98.77,-179.67 104.03,-188.86 108.62,-183.57\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-179.5C240,-179.5 151,-179.5 151,-179.5 145,-179.5 139,-173.5 139,-167.5 139,-167.5 139,-123.5 139,-123.5 139,-117.5 145,-111.5 151,-111.5 151,-111.5 240,-111.5 240,-111.5 246,-111.5 252,-117.5 252,-123.5 252,-123.5 252,-167.5 252,-167.5 252,-173.5 246,-179.5 240,-179.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M195.5,-222.91C195.5,-212.2 195.5,-200.62 195.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"199,-189.67 195.5,-179.67 192,-189.67 199,-189.67\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M371,-179.5C371,-179.5 282,-179.5 282,-179.5 276,-179.5 270,-173.5 270,-167.5 270,-167.5 270,-123.5 270,-123.5 270,-117.5 276,-111.5 282,-111.5 282,-111.5 371,-111.5 371,-111.5 377,-111.5 383,-117.5 383,-123.5 383,-123.5 383,-167.5 383,-167.5 383,-173.5 377,-179.5 371,-179.5\"/>\n<text text-anchor=\"start\" x=\"298.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"289\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"279.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"278\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M383.97,-222.91C375.42,-211.54 366.12,-199.18 357.54,-187.77\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"360.25,-185.55 351.45,-179.67 354.66,-189.76 360.25,-185.55\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M548,-187C548,-187 413,-187 413,-187 407,-187 401,-181 401,-175 401,-175 401,-116 401,-116 401,-110 407,-104 413,-104 413,-104 548,-104 548,-104 554,-104 560,-110 560,-116 560,-116 560,-175 560,-175 560,-181 554,-187 548,-187\"/>\n<text text-anchor=\"start\" x=\"409\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"445\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"443\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"433.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"428\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M437.4,-222.91C442.31,-214.2 447.56,-204.9 452.64,-195.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"455.78,-197.45 457.64,-187.02 449.68,-194.01 455.78,-197.45\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M461,-68C461,-68 364,-68 364,-68 358,-68 352,-62 352,-56 352,-56 352,-12 352,-12 352,-6 358,0 364,0 364,0 461,0 461,0 467,0 473,-6 473,-12 473,-12 473,-56 473,-56 473,-62 467,-68 461,-68\"/>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"375\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"365.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"360\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M455.18,-103.73C449.74,-94.97 443.99,-85.7 438.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"441.43,-74.95 433.18,-68.3 435.48,-78.64 441.43,-74.95\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M592,-68C592,-68 503,-68 503,-68 497,-68 491,-62 491,-56 491,-56 491,-12 491,-12 491,-6 497,0 503,0 503,0 592,0 592,0 598,0 604,-6 604,-12 604,-12 604,-56 604,-56 604,-62 598,-68 592,-68\"/>\n<text text-anchor=\"start\" x=\"519.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"510\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"499\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M505.45,-103.73C510.81,-94.97 516.48,-85.7 521.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"524.89,-78.66 527.12,-68.3 518.92,-75 524.89,-78.66\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M707.5,-306C707.5,-306 569.5,-306 569.5,-306 563.5,-306 557.5,-300 557.5,-294 557.5,-294 557.5,-235 557.5,-235 557.5,-229 563.5,-223 569.5,-223 569.5,-223 707.5,-223 707.5,-223 713.5,-223 719.5,-229 719.5,-235 719.5,-235 719.5,-294 719.5,-294 719.5,-300 713.5,-306 707.5,-306\"/>\n<text text-anchor=\"start\" x=\"565.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal length (cm) ≤ 5.95</text>\n<text text-anchor=\"start\" x=\"603\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"601\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"591.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"590\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M638.5,-341.91C638.5,-333.65 638.5,-324.86 638.5,-316.3\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"642,-316.02 638.5,-306.02 635,-316.02 642,-316.02\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M843,-298.5C843,-298.5 750,-298.5 750,-298.5 744,-298.5 738,-292.5 738,-286.5 738,-286.5 738,-242.5 738,-242.5 738,-236.5 744,-230.5 750,-230.5 750,-230.5 843,-230.5 843,-230.5 849,-230.5 855,-236.5 855,-242.5 855,-242.5 855,-286.5 855,-286.5 855,-292.5 849,-298.5 843,-298.5\"/>\n<text text-anchor=\"start\" x=\"768.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"755.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"start\" x=\"746\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 43]</text>\n<text text-anchor=\"start\" x=\"748\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>12&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M693.32,-341.91C709.56,-329.88 727.31,-316.73 743.44,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"745.76,-307.43 751.71,-298.67 741.59,-301.81 745.76,-307.43\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M687,-179.5C687,-179.5 590,-179.5 590,-179.5 584,-179.5 578,-173.5 578,-167.5 578,-167.5 578,-123.5 578,-123.5 578,-117.5 584,-111.5 590,-111.5 590,-111.5 687,-111.5 687,-111.5 693,-111.5 699,-117.5 699,-123.5 699,-123.5 699,-167.5 699,-167.5 699,-173.5 693,-179.5 687,-179.5\"/>\n<text text-anchor=\"start\" x=\"610.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"601\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"591.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"586\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M638.5,-222.91C638.5,-212.2 638.5,-200.62 638.5,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"642,-189.67 638.5,-179.67 635,-189.67 642,-189.67\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M818,-179.5C818,-179.5 729,-179.5 729,-179.5 723,-179.5 717,-173.5 717,-167.5 717,-167.5 717,-123.5 717,-123.5 717,-117.5 723,-111.5 729,-111.5 729,-111.5 818,-111.5 818,-111.5 824,-111.5 830,-117.5 830,-123.5 830,-123.5 830,-167.5 830,-167.5 830,-173.5 824,-179.5 818,-179.5\"/>\n<text text-anchor=\"start\" x=\"745.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"736\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"726.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"725\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 13&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>13&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M685.34,-222.91C698.96,-211.1 713.83,-198.22 727.4,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"729.97,-188.86 735.23,-179.67 725.38,-183.57 729.97,-188.86\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7d677d2c9bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adee6ed4"
      },
      "source": [
        "21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd5a6fc5",
        "outputId": "b5f4962d-066c-4954-df7e-5ed9ac8f271a"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a fully grown Decision Tree Classifier\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully grown Decision Tree Classifier: {accuracy_full:.2f}\")\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth = 3\n",
        "dt_max_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_max_depth_3.fit(X_train, y_train)\n",
        "y_pred_max_depth_3 = dt_max_depth_3.predict(X_test)\n",
        "accuracy_max_depth_3 = accuracy_score(y_test, y_pred_max_depth_3)\n",
        "print(f\"Accuracy of Decision Tree Classifier with max_depth=3: {accuracy_max_depth_3:.2f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of fully grown Decision Tree Classifier: 1.00\n",
            "Accuracy of Decision Tree Classifier with max_depth=3: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ecbca5e"
      },
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e9beb96",
        "outputId": "c469f304-7c64-4dea-967b-d003b444a73a"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with default min_samples_split (which is 2)\n",
        "dt_default_split = DecisionTreeClassifier(random_state=42)\n",
        "dt_default_split.fit(X_train, y_train)\n",
        "y_pred_default_split = dt_default_split.predict(X_test)\n",
        "accuracy_default_split = accuracy_score(y_test, y_pred_default_split)\n",
        "print(f\"Accuracy of Decision Tree Classifier with default min_samples_split: {accuracy_default_split:.2f}\")\n",
        "\n",
        "# Train a Decision Tree Classifier with min_samples_split = 5\n",
        "dt_min_samples_5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "dt_min_samples_5.fit(X_train, y_train)\n",
        "y_pred_min_samples_5 = dt_min_samples_5.predict(X_test)\n",
        "accuracy_min_samples_5 = accuracy_score(y_test, y_pred_min_samples_5)\n",
        "print(f\"Accuracy of Decision Tree Classifier with min_samples_split=5: {accuracy_min_samples_5:.2f}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier with default min_samples_split: 1.00\n",
            "Accuracy of Decision Tree Classifier with min_samples_split=5: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eafd7c9"
      },
      "source": [
        "23. Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0c40c4d",
        "outputId": "2b3f4a54-a612-4957-93e2-38b539c0a287"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier on unscaled data\n",
        "dt_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "dt_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = dt_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy of Decision Tree Classifier on unscaled data: {accuracy_unscaled:.2f}\")\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Decision Tree Classifier on scaled data\n",
        "dt_scaled = DecisionTreeClassifier(random_state=42)\n",
        "dt_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = dt_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy of Decision Tree Classifier on scaled data: {accuracy_scaled:.2f}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier on unscaled data: 1.00\n",
            "Accuracy of Decision Tree Classifier on scaled data: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd015c8"
      },
      "source": [
        "24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b2799fe",
        "outputId": "cc45236a-4341-45a0-a5e9-ce955c50b6c4"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create a OneVsRestClassifier with the Decision Tree Classifier\n",
        "ovr_classifier = OneVsRestClassifier(dt_classifier)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Accuracy of Decision Tree Classifier using One-vs-Rest strategy: {accuracy_ovr:.2f}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree Classifier using One-vs-Rest strategy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4dc9087"
      },
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "11a3e0c2",
        "outputId": "8ac5e5ca-824e-442c-e4ab-f96d88521bdc"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Train a Decision Tree Classifier (using the full dataset for feature importance)\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_iris, y_iris)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "\n",
        "# Create a pandas Series for better visualization\n",
        "feature_importance_series = pd.Series(feature_importances, index=X_iris.columns)\n",
        "\n",
        "# Display feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "display(feature_importance_series.sort_values(ascending=False))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "petal length (cm)    0.564056\n",
              "petal width (cm)     0.422611\n",
              "sepal length (cm)    0.013333\n",
              "sepal width (cm)     0.000000\n",
              "dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>petal length (cm)</th>\n",
              "      <td>0.564056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>petal width (cm)</th>\n",
              "      <td>0.422611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <td>0.013333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32030641"
      },
      "source": [
        "26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4759016a",
        "outputId": "0579f158-63e8-4666-c9cb-2153f8af9020"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import pandas as pd\n",
        "\n",
        "# Load the California Housing dataset (if not already loaded)\n",
        "housing = fetch_california_housing()\n",
        "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y_housing = pd.Series(housing.target)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_housing, y_housing, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an unrestricted Decision Tree Regressor\n",
        "dt_regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor_full.fit(X_train_h, y_train_h)\n",
        "y_pred_full_h = dt_regressor_full.predict(X_test_h)\n",
        "mse_full_h = mean_squared_error(y_test_h, y_pred_full_h)\n",
        "print(f\"Mean Squared Error of unrestricted Decision Tree Regressor: {mse_full_h:.2f}\")\n",
        "\n",
        "# Train a Decision Tree Regressor with max_depth = 5\n",
        "dt_regressor_max_depth_5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "dt_regressor_max_depth_5.fit(X_train_h, y_train_h)\n",
        "y_pred_max_depth_5_h = dt_regressor_max_depth_5.predict(X_test_h)\n",
        "mse_max_depth_5_h = mean_squared_error(y_test_h, y_pred_max_depth_5_h)\n",
        "print(f\"Mean Squared Error of Decision Tree Regressor with max_depth=5: {mse_max_depth_5_h:.2f}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of unrestricted Decision Tree Regressor: 0.53\n",
            "Mean Squared Error of Decision Tree Regressor with max_depth=5: 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91b1d4d9"
      },
      "source": [
        "27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f004b3af"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier to find the cost complexity pruning path\n",
        "path = DecisionTreeClassifier(random_state=42).cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Train Decision Tree Classifiers with different alpha values\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Calculate accuracy on training and testing sets for each alpha\n",
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "# Visualize the effect of alpha on accuracy\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
        "        drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
        "        drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce455d80"
      },
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "457851ad",
        "outputId": "165e0ceb-f214-4e7b-dbc5-8c35e6505a53"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc8ff64c"
      },
      "source": [
        "29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f5c2018",
        "outputId": "3bd95a00-cde6-4f84-a569-fa360c0f1a83"
      },
      "source": [
        "# Install seaborn\n",
        "%pip install seaborn matplotlib"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "2258e12a",
        "outputId": "d2b7f0ff-80c7-4974-ea30-3fdf9a0a8ea4"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b775557d"
      },
      "source": [
        "30. Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db303ac4",
        "outputId": "ca40f681-ba47-4a4e-9ccf-c69dc9ba4360"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y_iris = pd.Series(iris.target)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 3, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV instance\n",
        "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform GridSearchCV on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Evaluate the best model on the testing data\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "test_accuracy = best_dt_model.score(X_test, y_test)\n",
        "print(f\"Accuracy of the best model on the test data: {test_accuracy:.2f}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'min_samples_split': 10}\n",
            "Best cross-validation accuracy: 0.94\n",
            "Accuracy of the best model on the test data: 1.00\n"
          ]
        }
      ]
    }
  ]
}